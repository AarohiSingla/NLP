{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# What is Word Embedding?\n",
    "Word Embedding is a type of word representation that allows words with similar meaning to be understood by machine learning algorithms.  <br>\n",
    "\n",
    "\n",
    "There are various word embedding models available such as\n",
    "1. word2vec (Google)\n",
    "2. Glove (Stanford) \n",
    "3. fastest (Facebook)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where is Word Embedding used?\n",
    "\n",
    "1. Document clustering : The task of organizing a collection of documents, whose classification is unknown, into meaningful groups (clusters)\n",
    "\n",
    "\n",
    "2. Text classification\n",
    "Text classification (also known as text tagging or text categorization) is a process in which texts are sorted into categories. For example, you can classify news articles by topic, customer feedback by sentiment, support tickets by urgency, and so on.\n",
    "\n",
    "\n",
    "\n",
    "3. Natural language processing tasks. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is word2vec?\n",
    "\n",
    "word2vec is a two-layer network where there is input one hidden layer and output.\n",
    "\n",
    "# What word2vec does?\n",
    "Word2vec represents words in vector space representation. Words are represented in the form of vectors and placement is done in such a way that similar meaning words appear together and dissimilar words are located far away. This is also termed as a semantic relationship. Neural networks do not understand text instead they understand only numbers. Word Embedding provides a way to convert text to a numeric vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('law', 0.9333349466323853), ('general', 0.9253391623497009), ('policy', 0.9231358170509338), ('practice', 0.9205095171928406), ('media', 0.9205013513565063), ('agriculture', 0.9196840524673462), ('discussion', 0.9127441048622131), ('Cooper', 0.9100888967514038), ('Crean', 0.9081363081932068), ('tight', 0.9076606631278992)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#import the corpus abc which has been downloaded using nltk.download('abc').\n",
    "#Gensim is an open source Python library for natural language processing\n",
    "\n",
    "import nltk\n",
    "import gensim  # pip install gensim if not installed yet\n",
    "from nltk.corpus import abc # import abc corpus\n",
    "\n",
    "#Pass the files to the model word2vec which is imported using Gensim as sentences.\n",
    "model= gensim.models.Word2Vec(abc.sents())\n",
    "\n",
    "#Vocabulary is stored in the form of the variable\n",
    "X= list(model.wv.vocab)\n",
    "\n",
    "#Model is tested on sample word science as these files are related to science.\n",
    "data=model.most_similar('science')\n",
    "\n",
    "#here the similar word of \"science\" is predicted by the model.\n",
    "print(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
